# SFT Datasets

<font size=5><center><b> Table of Contents </b> </center></font>
- [SFT Datasets](#sft-datasets)
- [Awesome SFT Datasets](#awesome-sft-datasets)
  - [Caption](#caption)
  - [General VQA](#general-vqa)
  - [Mathematics](#mathematics)
  - [Chart \& Table](#chart--table)
  - [OCR](#ocr)
  - [Knowledge](#knowledge)
  - [Grounding](#grounding)
  - [Science](#science)
  - [Conversation](#conversation)
  - [Medical](#medical)
  - [GUI](#gui)
---

# Awesome SFT Datasets

## Caption

|   Name  |  Paper  |   Year   | Volume | Language | Modality |
|:--------|:-------:|:--------:|:------:|:--------:|:--------:|
| **TextCaps** | [TextCaps](https://arxiv.org/abs/2003.12462) | 2020 | 145K | en | Image |
| **ShareGPT4o** | [ShareGPT4o](https://sharegpt4o.github.io/) | 2024 | 200K | en & zh | Image |
| **CoCo Caption** | [CoCo Caption](https://arxiv.org/abs/1504.00325) | 2015 | 118K | en & zh | Image |
| **New YorkerCaptionContest** | [New YorkerCaptionContest](https://arxiv.org/abs/2209.06293) | 2022 | 148K | en | Image |
| **MMInstruct** | [MMInstruct](https://arxiv.org/abs/2407.15838) | 2024 | 161K | en & zh | Image |
| **Image Paragraph Captioning** | [Image Paragraph Captioning](https://arxiv.org/abs/1611.06607) | 2016 | 19K | en | Image |
| **ShareGPT4V-100K** | [ShareGPT4V-100K](https://arxiv.org/abs/2311.12793) | 2023 | 100K | en | Image |
| **Vript** | [Vript](https://arxiv.org/abs/2406.06040) | 2024 | 1.39M | en & zh | Video |
| **OpenVid** | [OpenVid](https://arxiv.org/abs/2407.02371) | 2024 | 1M | en | Video |
| **ShareGPT4o-Video** | [ShareGPT4o-Video](https://sharegpt4o.github.io/) | 2024 | 10K | en & zh | Video |
| **ShareGPT4-Video** | [ShareGPT4-Video](https://arxiv.org/abs/2406.04325) | 2024 | 40K | en & zh | Video |
| **VideoGPT+** | [VideoGPT+](https://arxiv.org/abs/2406.09418) | 2024 | 112K | en | Video |
| **MSR-VTT** | [MSR-VTT](https://openaccess.thecvf.com/content_cvpr_2016/papers/Xu_MSR-VTT_A_Large_CVPR_2016_paper.pdf) | 2016 | 200K | en | Video |

## General VQA

|   Name  |  Paper  |   Year   | Volume | Language | Modality |
|:--------|:-------:|:--------:|:------:|:--------:|:--------:|
| **GQA** | [GQA](https://arxiv.org/abs/1902.09506) | 2019 | 22M | en | Image |
| **VQAv2** | [VQAv2](https://arxiv.org/abs/1612.00837) | 2016 | 1M | en | Image |
| **ALLaVA** | [ALLaVA](https://arxiv.org/abs/2402.11684) | 2024 | 660K | en | Image |
| **SVIT** | [SVIT](https://arxiv.org/abs/2307.04087) | 2023 | 4.2M | en | Image |
| **LVIS-Instruct4V** | [LVIS-Instruct4V](https://arxiv.org/abs/2311.07574) | 2023 | 220K | en | Image |
| **Img-Diff** | [Img-Diff](https://arxiv.org/abs/2408.04594) | 2024 | 12K | en | Image |
| **Spot-the-diff** | [Spot-the-diff](https://arxiv.org/abs/1808.10584) | 2018 | 13K | en | Image |
| **NLVR2** | [NLVR2](https://arxiv.org/abs/1811.00491) | 2018 | 107K | en | Image |
| **VideoInstruct** | [VideoInstruct](https://arxiv.org/abs/2306.05424) | 2023 | 100K | en | Video |
| **EgoTaskQA** | [EgoTaskQA](https://arxiv.org/abs/2210.03929) | 2022 | 40K | en | Video |
| **MovieQA** | [MovieQA](https://arxiv.org/abs/1512.02902) | 2015 | 15K | en | Video |
| **CLEVERER** | [CLEVERER](https://arxiv.org/abs/1910.01442) | 2019 | 305K | en | Video |

## Mathematics

|   Name  |  Paper  |   Year   | Volume | Language | Modality |
|:--------|:-------:|:--------:|:------:|:--------:|:--------:|
| **GeoQA+** | [GeoQA+](https://aclanthology.org/2022.coling-1.130/) | 2022 | 12K | en | Image |
| **CLEVR-Math** | [CLEVR-Math](https://arxiv.org/abs/2208.05358) | 2022 | 68.3K | en | Image |
| **Super-CLEVR** | [Super-CLEVR](https://arxiv.org/abs/2212.00259) | 2022 | 600K | en | Image |
| **MapQA** | [MapQA](https://arxiv.org/abs/2211.08545) | 2022 | 800K | en | Image |
| **MAVIS** | [MAVIS](https://arxiv.org/abs/2407.08739) | 2024 | 1.39M | en | Image |
| **Geometry3K** | [Geometry3K](https://arxiv.org/abs/2105.04165) | 2021 | 3K | en | Image |
| **TallyQA** | [TallyQA](https://arxiv.org/abs/1810.12440) | 2018 | 28.8K | en | Image |
| **GEOS** | [GEOS](https://aclanthology.org/D15-1171/) | 2015 | 395K | en | Image |
| **UniGeo** | [UniGeo](https://arxiv.org/abs/2212.02746) | 2022 | 14.5K | en | Image |
| **GeomVerse** | [GeomVerse](https://arxiv.org/abs/2312.12241) | 2023 | 12K | en | Image |
| **CMM-Math** | [CMM-Math](https://arxiv.org/abs/2409.02834) | 2024 | 28K | zh | Image |
| **MathQA** | [MathQA](https://arxiv.org/abs/1905.13319) | 2019 | 37.2K | en | Image |

## Chart & Table

|   Name  |  Paper  |   Year   | Volume | Language | Modality |
|:--------|:-------:|:--------:|:------:|:--------:|:--------:|
| **ChartQA** | [Chartqa: A benchmark for question answering about charts with visual andlogical reasoning](https://arxiv.org/pdf/2203.10244) | 2022 | 32.7K | en | Image |
| **DVQA** | [Dvqa: Understanding data visualizations via question answering](https://openaccess.thecvf.com/content_cvpr_2018/papers/Kafle_DVQA_Understanding_Data_CVPR_2018_paper.pdf) | 2018 | 3.487M | en | Image |
| **PlotQA** | [Plotqa: Reasons over scientific plots](http://openaccess.thecvf.com/content_WACV_2020/papers/Methani_PlotQA_Reasoning_over_Scientific_Plots_WACV_2020_paper.pdf) | 2020 | 28M | en | Image |
| **FigureQA** | [FigureQA: An annotated figure dataset for visual reasoning](https://arxiv.org/pdf/1710.07300) | 2018 | 1M | en | Image |
| **TabMWP** | [Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning](https://arxiv.org/pdf/2209.14610) | 2023 | 38K | en | Image |
| **MMC-Inst** | [Mmc: Advancing multimodal chart understanding with large-scale instruction tuning](https://arxiv.org/pdf/2311.10774) | 2024 | 600K | en | Image |
| **UniChart** | [Unichart: A universal vision-language pretrained model for chart comprehension and reasoning](https://arxiv.org/pdf/2305.14761) | 2023 | 189K | en | Image |
| **MMTab** | [Multimodal table understanding](https://arxiv.org/pdf/2406.08100) | 2024 | 82K | en | Image |
| **ChartX** | [ChartX & ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning](https://arxiv.org/pdf/2402.12185) | 2024 | 6K | en | Image |

## OCR

|   Name  |  Paper  |   Year   | Volume | Language | Modality |
|:--------|:-------:|:--------:|:------:|:--------:|:--------:|
| **TextCaps** | [TextCaps](https://arxiv.org/abs/2003.12462) | 2020 | 28K | en | Image |
| **LSVT** | [LSVT](https://arxiv.org/abs/1909.07741) | 2019 | 450K | zh | Image |
| **RCTW-17** | [RCTW-17](https://arxiv.org/abs/1708.09585) | 2017 | 12K | zh | Image |
| **InfoVQA** | [InfoVQA](https://arxiv.org/abs/2104.12756) | 2021 | 30K | en | Image |
| **DocVQA** | [DocVQA](https://arxiv.org/abs/2007.00398) | 2020 | 50K | en | Image |
| **MPDocVQA** | [MPDocVQA](https://arxiv.org/abs/2212.05935) | 2022 | 46K | en | Image |

## Knowledge

|   Name  |  Paper  |   Year   | Volume | Language | Modality |
|:--------|:-------:|:--------:|:------:|:--------:|:--------:|
| **iNaturalist2018** | [iNaturalist](https://arxiv.org/abs/1707.06642) | 2017 | 675K | en | Image |
| **MovieNet** | [MovieNet](https://arxiv.org/abs/2007.10937) | 2020 | 1.1M | en | Image |
| **ART500K** | [DeepArt](https://dl.acm.org/doi/10.1145/3123266.3123405) | 2017 | 500K | en | Image |
| **KonIQ-10K** | [KonIQ-10k](https://arxiv.org/abs/1910.06180) | 2019 | 10K | en | Image |
| **WorldArt** | [WordArt](https://arxiv.org/abs/2208.00438) | 2022 | 4.8K | en | Image |
| **KVQA** | [KVQA](https://ojs.aaai.org/index.php/AAAI/article/view/4915) | 2019 | 183K | en | Image |
| **A-OKVQA** | [A-OKVQA](https://arxiv.org/abs/2206.01718) | 2022 | 25K | en | Image |
| **ViQuAE** | [ViQuAE](https://dl.acm.org/doi/10.1145/3477495.3531753) | 2022 | 3.7K | en | Image |
| **WIT** | [WIT](https://arxiv.org/abs/2103.01913) | 2021 | 37.6M | 108 languages | Image |
| **STEM-QA** | [STEM-QA](https://arxiv.org/abs/2402.17205) | 2024 | 1M | en | Image |

## Grounding

|   Name  |  Paper  |   Year   | Volume | Language | Modality |
|:--------|:-------:|:--------:|:------:|:--------:|:--------:|
| **RefCOCO** | [Paper](https://arxiv.org/abs/1511.02283) | 2016 | 130K | en | Image |
| **GPT4Gen-RD-BoxCoT** | [Paper](https://arxiv.org/abs/2306.15195) | 2023 | 5.9K | en | Image |
| **All-Seeing-V2** | [Paper](https://arxiv.org/abs/2402.19474) | 2024 | 127K | en | Image |
| **V3Det** | [Paper](https://arxiv.org/abs/2304.03752) | 2023 | 1.7M | en & zh | Image |
| **DsLMF** | [Paper](https://www.nature.com/articles/s41597-023-02322-9) | 2023 | 16K | en | Image |
| **COCO-ReM** | [Paper](https://arxiv.org/abs/2403.18819) | 2024 | 1.09M | en & zh | Image |
| **TolokaVQA** | [Paper](https://arxiv.org/abs/2309.16511) | 2023 | 45K | en | Image |

## Science

|   Name  |  Paper  |   Year   | Volume | Language | Modality |
|:--------|:-------:|:--------:|:------:|:--------:|:--------:|
| **AI2D** | [AI2D](https://arxiv.org/abs/1603.07396) | 2016 | 15K | en | Image |
| **ScienceQA**| [ScienceQA](https://arxiv.org/abs/2209.09513) | 2022 | 21K | en | Image |
| **TQA** | [TQA](https://openaccess.thecvf.com/content_cvpr_2017/papers/Kembhavi_Are_You_Smarter_CVPR_2017_paper.pdf) | 2017 | 26K | en | Image |
| **ChemVLM Data** | [ChemVLM Data](https://arxiv.org/abs/2408.07246) | 2024 | 257K | en & zh | Image |

## Conversation

| Name | Paper | Year | Volume | Language | Modality |
|:-----|:-----:|:----:|:------:|:--------:|:--------:|
| ALLaVA | [Allava: Harnessing gpt4v-synthesized data for a lite vision-language model](https://arxiv.org/pdf/2402.11684) | 2024 | 664K | en & zh | Image |
| Viet-ShareGPT4o | [Vintern-1b: An efficient multimodal large language model for vietnamese](https://arxiv.org/pdf/2408.12480) | 2024 | - | vi | Image |
| Cambrain-GPT4o | [Cambrian-1: A fully open, vision-centric exploration of multimodal llms](https://arxiv.org/pdf/2406.16860?) | 2024 | 10M | en | Image |
| RLAIF-V | [Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness](https://arxiv.org/pdf/2405.17220) | 2024 | 33.8k | en | Image |
| WildVision-GPT4o | [Wildvision: Evaluating vision-language models in the wild with human preferences](https://arxiv.org/pdf/2406.11069) | 2024 | 45.2k | en | Image |

## Medical

| Name | Paper | Year | Volume | Language | Modality |
|:-----|:-----:|:----:|:------:|:--------:|:--------:|
| PMC-VQA | [Pmc-vqa](https://arxiv.org/pdf/2305.10415) | 2023 | 227K | en | Image |
| VQA-RAD | [A dataset of clinically generated visual questions and answers about radiology images](https://www.nature.com/articles/sdata2018251.pdf) | 2018 | 3.5K | en | Image |
| PathVQA | [Pathvqa](https://arxiv.org/pdf/2003.10286) | 2020 | 32.8K | en | Image |
| VQA-Med | [Vqa-med](https://arodes.hes-so.ch/record/4214/files/Published%20version.pdf) | 2019 | 15K | en | Image |
| SLAKE | [Slake](https://arxiv.org/pdf/2102.09542) | 2021 | 14K | en & zh | Image |
| GMAI-VL | [Gmai-vl & gmai-vl-5.5 m](https://arxiv.org/pdf/2411.14522) | 2024 | 5.5M | en & zh | Image |

## GUI

| Name | Paper | Year | Volume | Language | Modality |
|:-----|:-----:|:----:|:------:|:--------:|:--------:|
| **Screen2Words** | [Screen2words](https://arxiv.org/abs/2108.03353) | 2021 | 112K | en | Image |
| **Widget-Caption** | [Widget captioning](https://arxiv.org/abs/2010.04295) | 2020 | 162.8K | en | Image |
| **UIBert** | [UIBert](https://arxiv.org/abs/2107.13731) | 2021 | 537K | en | Image |
| **RICO** | [Rico](https://dl.acm.org/doi/10.1145/3126594.3126651) | 2017 | 72.2K | en | Image |
| **AITW** | [AndroidInTheWild](https://arxiv.org/abs/2307.10088) | 2023 | 715 | en | Image |
| **Mind2Web** | [Mind2Web](https://arxiv.org/abs/2306.06070) | 2023 | 2K | en | Image |
| **SeeClick** | [SeeClick](https://arxiv.org/abs/2401.10935) | 2024 | 1.2K | en | Image |
| **ScreenQA** | [ScreenQA](https://arxiv.org/abs/2209.08199) | 2022 | 86K | en | Image |
| **AMEX** | [AMEX](https://arxiv.org/abs/2407.17490) | 2024 | 2.4M | en | Image |
| **Odyssey** | [GUI Odyssey](https://arxiv.org/abs/2406.08451) | 2024 | 7.7K | en | Image |
| **WebSight** | [WebSight](https://arxiv.org/abs/2403.09029) | 2024 | 2M | en | Image |
| **AndroidControl** | [AndroidControl](https://arxiv.org/abs/2406.03679) | 2024 | 15.3K | en | Image |
| **OmniACT** | [OmniACT](https://arxiv.org/abs/2402.17553) | 2024 | 9.8K | en | Image |
| **GUI-World** | [GUI-World](https://arxiv.org/abs/2406.10819) | 2024 | 12.4K | en | Video |
